AI Policy Research Summit, Stockholm,
November 2024
Roadmap for
AI Policy Research
2
Introduction
With Artificial intelligence (AI) development and adoption advancing at an
unprecedented pace, policymakers and regulators are encountering both
significant challenges and opportunities. The challenges emerge from the
disconnect between the fragmented & at times siloed policy & technology
landscapes, whilst the opportunities relate to novel insights and capabilities
afforded to decision-makers to strengthen the evidence base for sustainable
policy. While AI offers transformative potential, it also poses substantial risks, such
as biases, inequalities, and threats to privacy and security. In this context, AI policy
research has emerged as an essential guide to navigating the complex interplay
between technological innovation and societal impact. It ensures that
advancements in AI align with ethical, legal, and social priorities. AI policy research

advancements in AI align with ethical, legal, and social priorities. AI policy research
provides the evidence base needed to address these challenges, fostering
accountability, transparency, fairness, and inclusivity in AI governance. It also helps
anticipate future regulatory needs, bridge the gap between stakeholders, and
ensure that AI technologies are deployed responsibly and equitably, contributing
to sustainable development and the public good.
This roadmap, developed through collaborative discussions at the recent AI Policy
Research Summit, reflects a shared vision for advancing research on responsible
AI policy and governance. It emphasizes the critical role of policy research in
ensuring that AI development is guided by robust evidence, ethical
considerations, and a commitment to sustainability and inclusivity. By prioritizing
transparency, accountability, and the well-being of humans and the planet, this

transparency, accountability, and the well-being of humans and the planet, this
roadmap highlights how research can inform global approaches to AI governance,
addressing complex societal needs and ethical challenges. It serves as a guiding
framework for stakeholders across academia, industry, government, and civil
society to collaborate in generating actionable insights and evidence-based
strategies, noting that while evidence-based AI policy often draws on data-driven
research, it equally values critical theoretical insights and fundamental rights
approaches, ensuring a holistic understanding that extends beyond the purely
quantifiable.

Core Principles
Guiding principles for AI policy research should ensure rigorous adherence to
scientific and disciplinary methodologies. Moreover, these principles aim to
produce policy recommendations that not only reflect a nuanced understanding
of AI's potential but also emphasize the importance of balancing transformative
benefits with responsible and equitable implementation. By doing so, they
provide a framework that supports the ethical and effective integration of AI into
society. Key principles leading AI policy research include:
Human and Planetary Welfare: Research must prioritize the development of policies
2
that enhance individual and collective well-being while promoting human rights
and environmental sustainability. This includes systematically addressing direct and
indirect environmental impacts of AI systems, such as biodiversity, the ecosystem,
energy and water consumption, and electronic waste, and aligning AI deployment

energy and water consumption, and electronic waste, and aligning AI deployment
with the United Nations’ Sustainable Development Goals (SDGs). In particular, AI
policy research should address issues of exploitation within AI development. This
concerns both the hardware (mineral extraction) as well as exploitative work
practices where data workers often have little to no autonomy and where
psychologically and physically demanding tasks, e.g., removing toxic content, are
outsourced without proper compensation and labour rights.
Accountability and Transparency: AI policy research should advocate for, and
provide the tools and methods to support, decision-making processes that are
transparent, evidence-based, and accountable. This entails creating frameworks for
accessible, relevant, and reliable explainability in AI systems and ensuring that
policymakers and stakeholders understand the risks, assumptions, and trade-offs

policymakers and stakeholders understand the risks, assumptions, and trade-offs
involved in AI governance. The transparency principle underscores the importance
of traceability in AI system design, implementation, deployment, and, whenever
possible, reproducibility.
Inclusivity, Diversity, and Capacity Building: Effective AI policy research must address
disparities in access to AI technologies, their benefits, and AI-related decision-
making processes especially in underrepresented or historically marginalized
communities. This includes examining issues such as data bias, unequal distribution
of AI advancements and resources or AI risks, and systemic barriers to participation
in AI ecosystems on local, regional and global levels. Promoting diversity in AI
governance is essential to ensuring that AI serves a wide range of societal interests.

Ethical Research Practices and Institutional Responsibility: AI policy research and
collaborations must adhere to rigorous ethical standards that ensure innovation
aligns with societal well-being. This involves upholding principles of fairness,
institutional responsibility, accountability, human-centered values, human rights,
robustness, safety, and inclusiveness throughout the research process.
Researchers should be guided by established frameworks, such as those from
OECD and UNESCO, to maintain transparency, integrity, and methodological
rigour. By fostering trust and credibility, these practices ensure not only
responsible research but also institutional accountability and a commitment to
ethical governance in shaping the future of AI.
Ethical Governance: Governance should be guided by ethical considerations that
balance innovation with societal impact, following principles of fairness,
accountability, human-centered values, human rights, robustness, safety, and

accountability, human-centered values, human rights, robustness, safety, and
inclusiveness as advocated by principles and guidelines such as OECD and
UNESCO. AI policy research provides the much-needed scientific evidence to
support and refine these guidelines. By grounding governance principles in
scientific theories and empirical research, AI policy research ensures that ethical
frameworks remain actionable, measurable, and responsive to evolving
challenges, fostering responsible AI policy and governance.
Equitable economic growth: AI policy research should focus on ensuring that the
economic benefits of AI are distributed fairly across society, promoting
opportunities for underrepresented communities and fostering inclusive growth.
This includes addressing systemic economic inequalities exacerbated by AI
adoption, supporting policies that create equitable access to AI-driven markets
and resources, and encouraging sustainable market practices that prevent

and resources, and encouraging sustainable market practices that prevent
monopolistic dependencies. By prioritizing fairness and inclusivity in economic
frameworks, AI policy can contribute to shared prosperity and long-term
economic resilience.

Priorities for Impactful AI Policy Research
Addressing pressing challenges in AI policy requires focused and collaborative
research efforts to understand transformative benefits and ensure AI governance
aligns with societal needs. These efforts must bridge gaps between stakeholders,
anticipate future developments, and foster cross-border cooperation to guide
responsible and inclusive AI innovation. In particular, we identify the need for
research on:
Transboundary AI Governance: Effective AI governance requires exploring how
principles such as transparency, accountability, and trust can be interpreted,
understood, and implemented across borders, in particular in regional harmonised
2
initiatives, without assuming universal uniformity. Research should focus on
identifying flexible mechanisms for harmonising standards among diverse actors in
the AI ecosystem. This includes examining context-sensitive approaches to

the AI ecosystem. This includes examining context-sensitive approaches to
governance that respect local values and priorities while fostering international
cooperation. By exploring horizontal governance strategies, studies can clarify
operational norms and facilitate collaboration, even in the absence of globally
binding legislation.
Defining and Measuring the Benefits of AI: As AI continues to shape society, how do
we define and measure its benefits? AI promises to enhance well-being, efficiency,
and innovation, but who truly gains, and who is left behind? Are different benefits in
conflict, and can they be meaningfully qualified or quantified? Answering these
questions requires reflection on what it means for AI to be beneficial and how we
assess its true impact on individuals and communities.
Foresight and Proactive Regulation: AI policy research must anticipate technological
advancements and their implications to guide regulatory frameworks proactively

advancements and their implications to guide regulatory frameworks proactively
and leverage the use of AI to identify and monitor signals for positive change. This
includes conducting foresight exercises to predict emerging trends and their
societal impacts, enabling policymakers to prepare adaptable and forward-thinking
regulations. It also includes understanding the impact of AI narratives on public
attitudes towards AI and perception of humanity’s value and role in society.
Codes of Conduct: AI policy research can also inform the development of codes of
practice for different types of stakeholders, including researchers, developers,
deployers and policymakers, ensuring that governance evolves alongside innovation,
rather than in reaction to it.

Stakeholder Collaboration: Research in AI policy can identify best practices for
stakeholder engagement, develop frameworks for collaborative dialogue, and
create scenarios that align diverse interests. By bridging gaps between
stakeholders, research ensures that governance decisions are informed by
comprehensive, multi-perspective insights.
Sectoral Focus: Research can prioritize areas of high impact for individuals,
communities, and regions, such as sustainability, health, job access, or security,
ensuring that an AI solution is not taken for granted but part of a critical and
participatory process (that is, addressing "question zero"). Comparative analyses
of regulatory approaches from fields such as environmental policy and
healthcare governance can provide valuable insights.
ADMAP FOR AI POLICY RESEARCH

Guiding Actions and Roadmap
To develop a comprehensive, collaborative, and cross-border effort towards
advancing AI policy research, we propose the following immediate and long-term
actions:

Establishing a Community of Practice: Develop networks and
platforms for researchers, policymakers, and industry leaders to
share best practices, foster collaboration, and address
challenges in AI policy research.
Visiting AI Policy Fellowships: Create fellowship opportunities to
build stronger connections between researchers and
policymakers, enabling practical exchange of knowledge and
alignment of research with policy needs.
Annual AI Policy Summit: Host an annual forum to bring
together stakeholders for discussions, knowledge-sharing, and
collaborative reflection on AI policy research progress and
priorities.
Call to Action on AI Policy: Engage a broad range of stakeholders
through targeted campaigns to promote the adoption of ethical,
evidence-based AI policies and governance frameworks.
AI Policy Briefs: Develop and distribute concise, actionable, and
timely briefs to communicate research findings and policy
recommendations effectively to decision-makers.

recommendations effectively to decision-makers.
Student and Staff Exchange and Capacity Building: Facilitate
academic exchanges, exchanges between industry and
academia, and interdisciplinary programs to train future leaders
in AI policy research and governance, fostering a globally
connected and informed community.
AI Literacy: Promote AI literacy for people of all ages through
accessible media, educational programs, and public
engagement efforts, ensuring a widespread understanding of
AI’s impact, opportunities, and risks.

Commitments to Responsible Research and
Governance
We commit to advancing AI policy research that meets the growing need for
responsible governance, grounded in ethical, transparent, and evidence-
based practices to shape inclusive and trustworthy policies, including:
Responsible Research:
Transparency in methods, funding, and stakeholders.
Rigorous ethical standards and research integrity.
Advocacy for AI policies that avoid undue industry influence.
Responsible Governance:
Clear communication with policymakers to build trust.
Avoidance of undue influence from commercial entities.
Evidence-based consultation with relevant stakeholders to shape
inclusive policies.
Call to Action
The roadmap calls for unity among stakeholders to strengthen AI policy research
efforts. By engaging diverse groups, including policymakers, researchers, civil
society, and industry, this initiative seeks to ensure that AI advancements serve the

society, and industry, this initiative seeks to ensure that AI advancements serve the
public good, promoting accountability, sustainability, and inclusivity.
We invite your feedback to help refine these strategies, ensuring they effectively
support meaningful contributions to the global dialogue on AI policy and
governance, and shaping the role of AI policy research in addressing the
challenges and opportunities in this evolving field.

The AI dilemma (balancing safety with innovation) is universal, yet each jurisdiction pursues unique solutions. China, the EU, and the U.S. each take distinct approaches to harness AI’s potential while mitigating its risks. At the same time, AI ethics principles have emerged worldwide, revealing both common ground and notable differences across legal and ethical frameworks. This article examines how major jurisdictions regulate AI, the varied directions they take, and the underlying rationales driving their approaches.

I regularly update this article to reflect the rapid changes in the AI legal frameworks considered here (last update: November 2024).

Unlike in the realm of data privacy, the EU may not be the sole standard-setter for AI regulation to find the right balance for the AI dilemma. China and the U.S. are also competing for leadership, with each jurisdiction beginning its regulatory efforts at around the same time. Although this race to regulate artificial intelligence began before ChatGPT, the rise of generative AI has significantly accelerated global efforts. The divergence and debates in this AI race echo the varied approaches seen in data privacy regulation, as each jurisdiction pursues its own path based on national priorities and values.

Table of Contents

The AI Dilemma: AI Regulation in China, EU & the U.S.
What is the AI Dilemma?
The EU AI Act: A Risk-Based, Comprehensive Approach
The U.S.: Guidelines and Narrow Bills
China AI Regulation: Balancing Development and State control
China’s Agile Approach to Regulating AI
China AI Laws Favoring AI Adoption
Determination to Maintain State Control
Who will solve the AI Dilemma in Regulation?
Executive Summary: Diverse Solutions To The AI Dilemma

Who will solve the AI Dilemma in Regulation?
Executive Summary: Diverse Solutions To The AI Dilemma
This article provides a comprehensive comparison of the AI regulatory approaches taken by China, the EU, and the U.S. to solve the AI dilemma. Each shaped by unique national goals, values, and socio-political factors. While the EU’s AI Act seeks to set a global standard by focusing on risk-based safeguards for fundamental rights, it may struggle to keep up with AI’s rapid evolution. The U.S. adopts a flexible, sector-specific approach that promotes innovation, though it risks losing influence due to its lack of a centralized framework. China, meanwhile, balances agile regulation with state control, rapidly adapting its policies to support AI adoption while aligning AI development with government objectives. As global powers grapple with AI’s societal risks and potential, the race to establish international norms to solve the AI dilemma is still wide open.

What Is The AI Dilemma?
The AI dilemma centers on balancing safety with innovation. On the one hand, stringent AI laws can stifle innovation; on the other, unchecked innovation can pose risks to society and individuals.

Each jurisdiction may approach this balance differently, guided by broader national goals, cultural norms, and socio-political contexts. This diversity reflects unique national priorities around issues such as privacy, security, economic competitiveness, and technological leadership.

Ultimately, addressing the AI dilemma in regulation requires each country to find an optimal path that fosters technological advancement and its benefits while safeguarding against potential risks and aligning with the nation’s values and objectives.

AI dilemma
The EU AI Act: A Risk-Based, Comprehensive Approach

AI dilemma
The EU AI Act: A Risk-Based, Comprehensive Approach

With the AI Act, the EU aims to establish a global benchmark for AI regulation, similar to how the GDPR set standards for data privacy. Like the GDPR, the AI Act is a regulation – meaning it will be directly applicable across all EU Member States without requiring national legislation.

The AI Act categorizes AI systems based on their risk to society, with each category triggering different compliance requirements. Certain AI systems, such as social scoring and live facial recognition in public spaces, are outright prohibited. High-risk systems – often used in HR or law enforcement – bear the majority of the regulatory burden, particularly for providers of these AI solutions. These systems will need to undergo Fundamental Rights Impact Assessments, similar to the GDPR’s Data Protection Impact Assessments, reflecting the EU’s emphasis on protecting fundamental rights, including data privacy.

The EU AI Act also includes transparency requirements for AI systems that end-users might mistake for non-AI content, such as chatbots or other AI-generated information. Foundation models, like ChatGPT, are subject to additional regulations, particularly around transparency. Providers must disclose details about these models’ functioning, design, and guidelines for safe integration into organizational solutions. Finally, AI systems outside of these specified categories – such as spam filters and video games – are not in the scope of the EU AI Act.

The EU’s approach with the AI Act is intentionally broad, aiming to cover all sectors much like the GDPR. By being early to establish a comprehensive framework, the EU hopes the Act will serve as a global model, influencing practices worldwide—even though its formal scope is the EU internal market. The Act’s risk-based approach requires companies to identify and mitigate AI-related risks proactively. While the AI Act doesn’t directly grant individual rights as the GDPR does, its ultimate goal is to protect fundamental rights, a core driver that distinguishes the EU’s approach from those of China and the U.S.

The EU has prioritized swift action on AI regulation, a point that European Commissioner Thierry Breton has proudly emphasized. A key ambition of the AI Act is to set a global standard for AI regulation.

The U.S.: Guidelines And Narrow Bills

The U.S.: Guidelines And Narrow Bills

The U.S. approach to AI regulation is more piecemeal and sector-specific, with a strong emphasis on voluntary compliance – quite different from the EU’s comprehensive strategy. The underlying rationale is that AI technology still needs space to grow and develop before sweeping, binding regulations are put in place. Both government and private organizations have published various AI ethical principles and guidelines, and there are bills proposed at both federal and state levels. However, nothing as broad or centralized as the EU AI Act is currently on the horizon. This lightweight, sectoral approach is similar to the U.S. stance on data privacy regulation.

At the federal level, the U.S. Executive Order on AI has garnered significant attention. Issued by President Biden, it aims to promote ethical, safe, and trustworthy AI development, focusing on American values such as privacy, civil rights, and civil liberties. The order provides guidelines for federal agencies, balancing the need for AI innovation with responsible usage to align with public interest and national values across both government and private sectors. While not legally binding, it broadly directs agencies to procure and operate only responsible AI systems, and it places transparency and accountability requirements on companies developing these systems. The order also includes an express call for Congress to pass bipartisan legislation on AI and privacy. However, questions remain about how the order will be enforced.

Several bills are also being considered, most on a relatively narrow scope. At the federal level, a growing number of proposals arrive, one of the latest would require AI companies to disclose copyrighted training data. At the state level, one in California proposes rules for automated decision-making technology. Florida may strengthen transparency over AI-generated content for political ads.

California’s recent AI bill (SB 1047) offered a new perspective on the U.S. approach, emphasizing stringent safety standards specifically for high-cost AI models. However, Governor Newsom vetoed it in September 2024, citing its potential to stifle innovation and advocating instead for a balanced, risk-based regulation model similar to the EU’s approach. This development underscores the evolving debate within the U.S. on regulating AI without hindering its progress. For further details on California’s AI bill SB 1047, see my comments here.

The risk of the U.S.’s wait-and-see approach is that it may diminish its influence in the global regulatory debate. U.S. laws – and company practices – may ultimately be shaped by the so-called “Brussels Effect,” where companies adapt to comply with the EU AI Act, which will, in effect, set the standard for many U.S. requirements as well.

China AI Regulation: Balancing Development And State Control

Against this backdrop, China does support its AI sector through various policy tools (see below). One could say this support is part of a broader Chinese policy encouraging domestic development and Chinese actors. We can see it in China’s broader legal framework on tech, such as China data privacy law or the restrictions on cross-border data transfers in PIPL. Of course, the law is one of the country’s policy tools to support its objective of global AI dominance.

The Chinese government has set a clear objective: to make China the global AI leader by 2030. In this race for AI dominance, China’s primary competitor is the U.S., and the competition extends beyond research output to include export restrictions and espionage concerns, both critical to national security.

To advance its AI sector, China uses a range of policy tools that align with its broader strategy to promote domestic growth and strengthen Chinese tech companies. This approach is reflected in China’s broader legal framework for technology, including its data privacy laws and the cross-border data transfer restrictions in the Personal Information Protection Law (PIPL). Legislation is thus one of China’s strategic levers in its pursuit of global AI leadership.

In the remainder of this article, I’ll illustrate how China’s AI regulation aligns with this overarching goal.

China has positioned itself as an early mover in AI regulation, a contrast to its approach in data privacy, where it largely follows the EU’s model (particularly regarding consumer privacy, rather than citizen privacy). With AI law, however, China is staking out its own approach, stepping in early while the EU and the U.S. are still shaping their frameworks.

China’s Agile Approach To Regulating AI

China has adopted an agile approach to AI regulation, swiftly enacting policies on specific areas as they emerge. Typically, regulations are initially published as drafts, with amendments made following discussions with private sector stakeholders and academia. A notable example of this iterative process is China’s rules on generative AI, which were first proposed in April 2023 and finalized by August 2023.

This agile method offers both advantages and drawbacks. On the plus side, China’s AI legal framework is able to adapt quickly to new technologies, like generative AI. In contrast, Europe’s response to the sudden rise of ChatGPT required the AI Act drafters to hurriedly incorporate new provisions, which nearly derailed negotiations. However, the rapid, evolving nature of China’s AI laws means they are not yet fully stabilized, creating gray areas for companies navigating compliance. Additionally, the lack of clarity on enforcement mechanisms adds a layer of uncertainty for businesses operating under these emerging regulations.

China AI Laws Favoring AI Adoption

China AI Laws Favoring AI Adoption

Feedback on the April draft led to a softening of several requirements, creating a more favorable regulatory environment for AI companies. For instance, what was initially a strict obligation to ensure outputs were accurate was revised to a “best effort” standard. Additional AI regulations address algorithmic recommendations and deep synthesis.

High-level government directives also shape a legal landscape that supports AI adoption. In November 2023, a Beijing court ruled that AI-generated content could be eligible for copyright protection – a stark contrast to the U.S., where similar claims have been dismissed, and the EU, where the AI Act’s copyright provisions may present challenges for AI developers. Although this ruling came from a lower-level court, it signals a supportive stance toward AI in China. However, this approach may also raise questions, such as whether it could stifle human creativity, which AI still relies on to some extent. Overall, China’s regulatory approach underscores its commitment to promoting AI adoption and development.

Determination To Maintain State Control

Determination To Maintain State Control

A defining feature of China’s AI regulation is the emphasis on maintaining state control, which can sometimes clash with the goal of fostering rapid AI adoption. This need for control was recently reaffirmed by Xi Jinping, underscoring the importance of ensuring AI development aligns with the “correct political direction.” While certain regulatory requirements have been softened to support AI growth, these political obligations remain firmly in place, particularly for public-facing AI systems. This framework allows room for censorship and selective enforcement, distinguishing China’s approach from the EU and U.S., where respect for individual rights and democratic values is embedded in both emerging laws and ethical guidelines for AI.

State control over AI development in China also extends beyond legislation. Through significant public investment, including state-backed venture capital funds, the government influences AI companies’ development priorities, guiding them toward alignment with national objectives.

Who Will Solve The AI Dilemma In Regulation?

In this article, we explored the three distinct approaches that China, the EU, and the U.S. have taken toward AI law. Yet, it’s too early to determine who, if anyone, will ultimately set the global standard. If we look to privacy law for comparison, the EU has led the way, with GDPR now widely regarded as the global model for data privacy legislation. So far, however, no equivalent exists for AI regulation.

The EU aims for the AI Act to be an overarching framework, classifying and regulating AI systems according to their societal risks. At its core is the protection of fundamental rights. However, the challenge for the EU lies in its regulatory agility: AI technology evolves rapidly, as demonstrated by generative AI like ChatGPT, which emerged after the AI Act was proposed in 2021 and required substantial amendments to the draft.

The U.S., by contrast, favors guidelines and sector-specific bills to promote AI safety while leaving room for innovation. With no comprehensive federal privacy law yet in place, many aspects of AI regulation remain unaddressed. This wait-and-see approach allows flexibility but also risks sidelining the U.S. in the global debate, lacking a cohesive framework to offer as an international model.

China’s approach may be the most agile, adopting targeted policies to address specific AI issues and enacting laws quickly. This responsiveness enables China to react to new AI developments and could position it as a model for international AI laws. However, the potential lack of stability in its regulatory framework may create uncertainty for companies, leading to gray areas that could hinder development and investment.

These jurisdictions have each taken a unique stance in navigating the AI dilemma, but in 2024, the landscape remains fluid and evolving. In future analyses, I will delve deeper into their rationales and the potential paths forward. For now, it’s clear that establishing global standards in AI regulation may prove far more challenging than it was for data privacy.

